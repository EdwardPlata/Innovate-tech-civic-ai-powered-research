# Scout Data Discovery Platform

A comprehensive platform for exploring NYC Open Data using AI-powered data discovery and relationship analysis.

## ğŸ¯ First Time Here?

Run the automated setup script:
```bash
./setup.sh
```

This will check your system, install dependencies, and get you started in minutes!

## ğŸš€ Quick Start

### Deployment Options

Choose your preferred deployment method:

#### Option 1: Simple Run Script (Recommended)
```bash
# Quick start with automated setup
./run.sh

# With optimization
./run.sh --optimize
```

#### Option 2: Standard Startup Script
```bash
# Direct startup
./start_scout.sh
```

#### Option 3: Docker Deployment
```bash
# Using Docker Compose
docker-compose up -d

# Access the application
# Frontend: http://localhost:8501
# Backend: http://localhost:8080
```

See [Docker Deployment Guide](DOCKER_DEPLOYMENT.md) for detailed Docker instructions.

### Access the Application
- **Frontend (Main UI)**: http://localhost:8501
- **Backend API**: http://localhost:8080
- **API Documentation**: http://localhost:8080/docs

## ğŸ“‹ Deployment Scripts

### Main Scripts

**`run.sh`** - Simplified deployment script
```bash
./run.sh              # Start services
./run.sh --optimize   # Start with optimization
```

**`start_scout.sh`** - Full-featured startup script
```bash
./start_scout.sh           # Start services
./start_scout.sh --status  # Check status
./start_scout.sh --stop    # Stop services
./start_scout.sh --help    # Show help
```

### Optimization Scripts

Located in `scripts/` directory:

**`optimize-deps.sh`** - Manage dependencies
```bash
./scripts/optimize-deps.sh                  # Check/install dependencies
./scripts/optimize-deps.sh --check-outdated # Show outdated packages
./scripts/optimize-deps.sh --clean-cache    # Clean pip cache
```

**`optimize-cache.sh`** - Manage cache
```bash
./scripts/optimize-cache.sh --clean-old    # Clean old files
./scripts/optimize-cache.sh --clean-all    # Clean all cache
./scripts/optimize-cache.sh --stats        # Show statistics
```

**`health-check.sh`** - Verify services
```bash
./scripts/health-check.sh              # Basic health check
./scripts/health-check.sh --verbose    # Detailed output
./scripts/health-check.sh --check-deps # Check dependencies
```

See [Scripts Documentation](scripts/README.md) for detailed usage.

## ğŸ—ï¸ Architecture

```
Innovate-tech-civic-ai-powered-research/
â”œâ”€â”€ scout_data_discovery/     # Core Scout library
â”‚   â”œâ”€â”€ src/                  # Scout modules
â”‚   â”œâ”€â”€ examples/             # Usage examples & notebooks
â”‚   â”œâ”€â”€ tests/                # Unit tests
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ backend/                  # FastAPI backend service
â”‚   â”œâ”€â”€ main.py              # API endpoints
â”‚   â”œâ”€â”€ api/                 # Organized API modules
â”‚   â”‚   â”œâ”€â”€ models.py        # Pydantic models
â”‚   â”‚   â”œâ”€â”€ routes/          # Route handlers
â”‚   â”‚   â”œâ”€â”€ services/        # Business logic
â”‚   â”‚   â””â”€â”€ utils/           # Helper functions
â”‚   â”œâ”€â”€ cache_manager.py     # Caching system
â”‚   â”œâ”€â”€ api_config.py        # Configuration
â”‚   â”œâ”€â”€ run_server.py        # Server runner
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/                 # Streamlit web application
â”‚   â”œâ”€â”€ app.py               # Main Streamlit app
â”‚   â”œâ”€â”€ components/          # UI components
â”‚   â”‚   â”œâ”€â”€ backend_manager.py
â”‚   â”‚   â”œâ”€â”€ ai_analyst_component.py
â”‚   â”‚   â””â”€â”€ visualization_utils.py  # Optimized visualizations
â”‚   â”œâ”€â”€ run_app.py           # App runner
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ AI_Functionality/         # AI analysis modules
â”‚   â”œâ”€â”€ core/                # Core AI components
â”‚   â”œâ”€â”€ providers/           # AI provider integrations
â”‚   â””â”€â”€ docs/                # AI documentation
â”œâ”€â”€ scripts/                 # Deployment & optimization scripts
â”‚   â”œâ”€â”€ optimize-deps.sh    # Dependency management
â”‚   â”œâ”€â”€ optimize-cache.sh   # Cache optimization
â”‚   â”œâ”€â”€ health-check.sh     # Service health checks
â”‚   â””â”€â”€ README.md           # Scripts documentation
â”œâ”€â”€ docs/                    # General documentation
â”‚   â”œâ”€â”€ QUICK_START_GUIDE.md
â”‚   â”œâ”€â”€ USAGE.md
â”‚   â”œâ”€â”€ PERFORMANCE_OPTIMIZATION_GUIDE.md
â”‚   â””â”€â”€ BACKEND_ORGANIZATION.md
â”œâ”€â”€ transcripts/             # Implementation history
â”‚   â””â”€â”€ [various implementation summaries]
â”œâ”€â”€ subrepos/                # External integrations
â”‚   â””â”€â”€ n8n-workflows/
â”œâ”€â”€ run.sh                   # Simplified deployment script
â”œâ”€â”€ start_scout.sh           # Full-featured startup script
â”œâ”€â”€ docker-compose.yml       # Docker orchestration
â”œâ”€â”€ Dockerfile.backend       # Backend container image
â”œâ”€â”€ Dockerfile.frontend      # Frontend container image
â”œâ”€â”€ DOCKER_DEPLOYMENT.md     # Docker deployment guide
â””â”€â”€ README.md                # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- pip or conda for package management

### 1. Install Dependencies

```bash
# Install Scout Data Discovery core
cd scout_data_discovery
pip install -r requirements.txt

# Install backend dependencies
cd ../backend
pip install -r requirements.txt

# Install frontend dependencies
cd ../frontend
pip install -r requirements.txt
```

### 2. Launch the Platform

```bash
# Start from the frontend directory
cd frontend
python run_app.py
```

The web app will be available at: http://localhost:8501

### ğŸ¯ Integrated Backend Management

The backend API server is now **managed directly from the frontend**:

- âœ… **Automatic Start**: Click "ğŸš€ Start Backend Server" in the web interface
- âœ… **Real-time Status**: Backend status shown in the sidebar
- âœ… **Easy Control**: Start/stop backend from the UI
- âœ… **Health Monitoring**: Automatic health checks and status updates

No need to manually start the backend - everything is controlled from the web interface!

## âœ¨ Features

### ğŸ” Scout Data Discovery Library
- **Dataset Discovery**: Search across NYC Open Data with intelligent filtering
- **Quality Assessment**: 5-dimensional quality scoring (completeness, consistency, accuracy, timeliness, usability)
- **Relationship Analysis**: AI-powered discovery of dataset connections
- **Network Visualization**: Interactive graphs showing dataset relationships
- **Export Capabilities**: Multiple formats for further analysis

### ğŸŒ Web Interface
- **Interactive Dashboard**: Overview of recently updated datasets
- **Advanced Search**: Multi-term search with filtering and sorting
- **Quality Assessment**: Visual quality scoring with detailed breakdowns
- **Relationship Mapping**: Network graphs showing dataset connections
- **Data Sample Viewer**: Preview dataset contents before download
- **Export Options**: Download samples and analysis results

### ğŸ”Œ API Backend
- **RESTful API**: Clean endpoints for all Scout functionality
- **Async Processing**: Background tasks for long-running operations
- **Caching**: Intelligent caching for improved performance
- **CORS Support**: Cross-origin requests from frontend
- **Auto Documentation**: Swagger/OpenAPI docs at `/docs`

## ğŸ“Š Use Cases

### For Data Analysts
- **Discover Related Datasets**: Find complementary data sources
- **Assess Data Quality**: Make informed decisions about data usage
- **Integration Planning**: Identify datasets suitable for joining

### For Researchers
- **Literature Discovery**: Find relevant datasets for research
- **Data Landscape Analysis**: Understand available data domains
- **Quality Benchmarking**: Compare data quality across sources

### For City Officials
- **Data Inventory**: Comprehensive view of available open data
- **Usage Insights**: Understand popular datasets and trends
- **Quality Monitoring**: Track data quality across departments

## ğŸ› ï¸ API Endpoints

### Dataset Operations
- `GET /api/datasets/top-updated` - Recently updated datasets
- `POST /api/datasets/search` - Search datasets
- `GET /api/datasets/{id}/quality` - Quality assessment
- `GET /api/datasets/{id}/sample` - Data sample

### Relationship Analysis
- `POST /api/datasets/relationships` - Find related datasets
- `GET /api/network/visualization/{id}` - Network graph data

### System
- `GET /api/stats` - API usage statistics
- `GET /api/categories` - Available dataset categories

## ğŸ”§ Configuration

### Backend Configuration
Edit `backend/main.py` for API settings:
- Rate limiting
- Cache duration
- Sample sizes
- Worker threads

### Frontend Configuration
Edit `frontend/app.py` for UI settings:
- API endpoints
- Cache TTL
- Display options
- Visualization parameters

## ğŸ“ˆ Performance

### Recent Optimizations
- **Visualization Caching**: Charts cached for 10 minutes (60% faster)
- **Optimized Table Rendering**: 80% improvement for large datasets
- **Efficient Network Graphs**: 75% faster with 50% less memory
- **Smart Data Limiting**: Prevents browser slowdown
- See [Performance Optimization Guide](docs/PERFORMANCE_OPTIMIZATION_GUIDE.md) for details

### Core Features
- **Intelligent Caching**: Results cached with TTL management
- **Background Processing**: Long operations run asynchronously
- **Progressive Loading**: Large datasets loaded in chunks
- **Connection Pooling**: Efficient API connections

### Scalability
- **Stateless Design**: Easy horizontal scaling
- **Organized Backend**: Modular structure ready for growth
- **Database Ready**: Can integrate with PostgreSQL/Redis
- **Container Ready**: Full Docker support with docker-compose orchestration
- **Deployment Scripts**: Automated setup and optimization scripts

## ğŸ§ª Development

### Running Tests
```bash
# Scout library tests
cd scout_data_discovery
python -m pytest tests/ -v

# Backend tests (if implemented)
cd backend
python -m pytest tests/ -v
```

### Development Mode
Both frontend and backend support hot reload:

```bash
# Backend (auto-reload on code changes)
cd backend
uvicorn main:app --reload

# Frontend (auto-reload on code changes)
cd frontend
streamlit run app.py
```

## ğŸ“š Examples

### Using the Scout Library
```python
from src.scout_discovery import ScoutDataDiscovery

# Initialize Scout
scout = ScoutDataDiscovery(log_level="INFO")

# Search for datasets
datasets = scout.search_datasets(["311", "health"])

# Assess quality
quality = scout.assess_dataset_quality(dataset_id)

# Find relationships
from src.dataset_relationship_graph import DatasetRelationshipGraph
graph = DatasetRelationshipGraph()
graph.add_datasets(datasets)
relationships = graph.calculate_relationships()
```

### Using the API
```python
import requests

# Search datasets
response = requests.post("http://localhost:8000/api/datasets/search",
                        json={"search_terms": ["311"], "limit": 10})
datasets = response.json()

# Get quality assessment
quality = requests.get(f"http://localhost:8000/api/datasets/{dataset_id}/quality")
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“š Documentation

Comprehensive documentation is available in the `/docs` folder:

- **[Quick Start Guide](docs/QUICK_START_GUIDE.md)** - Get up and running quickly
- **[Usage Guide](docs/USAGE.md)** - Detailed usage instructions
- **[Performance Optimization Guide](docs/PERFORMANCE_OPTIMIZATION_GUIDE.md)** - Optimization strategies
- **[Backend Organization](docs/BACKEND_ORGANIZATION.md)** - Backend code structure
- **[Scout Technical Analysis](docs/Scout_Technical_Analysis.md)** - Deep dive into Scout methodology

Implementation history and change logs are in `/transcripts`.

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built on [Scout](https://scout.tsdataclinic.com/) methodology
- Powered by [NYC Open Data](https://opendata.cityofnewyork.us/)
- Uses [Socrata Discovery API](https://dev.socrata.com/)

---

For detailed usage examples, see the `scout_data_discovery/examples/` directory.