{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5ae13d",
   "metadata": {},
   "source": [
    "# Scout Data Discovery Platform Exploration\n",
    "\n",
    "## Overview\n",
    "This notebook explores Scout (https://scout.tsdataclinic.com/), a next-generation data discovery platform developed by The Data Clinic. Scout provides an innovative approach to browsing open data portals with enhanced discoverability features, dataset recommendations, and automated data curation capabilities.\n",
    "\n",
    "## What is Scout?\n",
    "Scout is designed to solve the problem of data discovery in large open data portals. Unlike traditional catalog browsing, Scout offers:\n",
    "\n",
    "- **Intelligent Dataset Recommendations**: Uses machine learning to suggest related datasets\n",
    "- **Enhanced Search Capabilities**: Semantic search across dataset descriptions and metadata\n",
    "- **Data Quality Insights**: Automated assessment of dataset completeness and usability\n",
    "- **Collection Creation**: Ability to curate and share collections of related datasets\n",
    "- **Cross-Portal Discovery**: Aggregates data from multiple Socrata-powered open data portals\n",
    "\n",
    "## Architecture\n",
    "Scout consists of:\n",
    "- **Frontend**: React-based web application for interactive data exploration\n",
    "- **Backend**: NestJS API server with PostgreSQL database\n",
    "- **Search Engine**: OpenSearch for fast, semantic dataset discovery\n",
    "- **Data Pipeline**: Automated ingestion from 120+ open data portals worldwide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da414d",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bafbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for web scraping and API interaction\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# For web scraping Scout's interface\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "# Data analysis and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For handling large datasets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Requests version: {requests.__version__}\")\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede36a0a",
   "metadata": {},
   "source": [
    "## 2. Explore Scout API Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f9c1a",
   "metadata": {},
   "source": [
    "## 3. Extract Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c63648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoutDatasetExtractor:\n",
    "    \"\"\"\n",
    "    Extract dataset metadata using Socrata Discovery API (Scout's underlying data source)\n",
    "    Since Scout aggregates Socrata portals, we can access the same data through Socrata's API\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.socrata_discovery_url = \"http://api.us.socrata.com/api/catalog/v1\"\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def get_nyc_datasets(self, limit: int = 100, offset: int = 0) -> Dict:\n",
    "        \"\"\"\n",
    "        Get NYC datasets using Socrata Discovery API (same data Scout uses)\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'domains': 'data.cityofnewyork.us',\n",
    "            'search_context': 'data.cityofnewyork.us',\n",
    "            'limit': limit,\n",
    "            'offset': offset\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(self.socrata_discovery_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def extract_dataset_metadata(self, dataset: Dict) -> Dict:\n",
    "        \"\"\"Extract key metadata from a dataset record\"\"\"\n",
    "        resource = dataset.get('resource', {})\n",
    "        classification = dataset.get('classification', {})\n",
    "        \n",
    "        return {\n",
    "            'id': resource.get('id'),\n",
    "            'name': resource.get('name'),\n",
    "            'description': resource.get('description'),\n",
    "            'attribution': resource.get('attribution'),\n",
    "            'type': resource.get('type'),\n",
    "            'updatedAt': resource.get('updatedAt'),\n",
    "            'createdAt': resource.get('createdAt'),\n",
    "            'download_count': resource.get('download_count'),\n",
    "            'page_views_total': resource.get('page_views', {}).get('page_views_total', 0),\n",
    "            'columns_count': len(resource.get('columns_name', [])),\n",
    "            'columns_names': resource.get('columns_name', []),\n",
    "            'columns_field_names': resource.get('columns_field_name', []),\n",
    "            'columns_datatypes': resource.get('columns_datatype', []),\n",
    "            'domain_category': classification.get('domain_category'),\n",
    "            'domain_tags': classification.get('domain_tags', []),\n",
    "            'categories': classification.get('categories', []),\n",
    "            'tags': classification.get('tags', [])\n",
    "        }\n",
    "    \n",
    "    def get_comprehensive_dataset_list(self, max_datasets: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Get a comprehensive list of NYC datasets with metadata\"\"\"\n",
    "        all_datasets = []\n",
    "        offset = 0\n",
    "        limit = 100\n",
    "        \n",
    "        print(\"Fetching NYC datasets from Socrata Discovery API...\")\n",
    "        \n",
    "        while len(all_datasets) < max_datasets:\n",
    "            print(f\"Fetching batch: offset={offset}, current total={len(all_datasets)}\")\n",
    "            \n",
    "            data = self.get_nyc_datasets(limit=limit, offset=offset)\n",
    "            \n",
    "            if 'error' in data:\n",
    "                print(f\"Error: {data['error']}\")\n",
    "                break\n",
    "                \n",
    "            results = data.get('results', [])\n",
    "            if not results:\n",
    "                print(\"No more results available\")\n",
    "                break\n",
    "            \n",
    "            for dataset in results:\n",
    "                metadata = self.extract_dataset_metadata(dataset)\n",
    "                all_datasets.append(metadata)\n",
    "            \n",
    "            offset += limit\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            if len(results) < limit:\n",
    "                break\n",
    "        \n",
    "        print(f\"Collected {len(all_datasets)} datasets\")\n",
    "        return pd.DataFrame(all_datasets)\n",
    "\n",
    "# Initialize extractor and get dataset metadata\n",
    "extractor = ScoutDatasetExtractor()\n",
    "\n",
    "# Get sample datasets first\n",
    "print(\"=== SAMPLE DATASET EXTRACTION ===\")\n",
    "sample_data = extractor.get_nyc_datasets(limit=10)\n",
    "\n",
    "if 'error' not in sample_data:\n",
    "    print(f\"Successfully retrieved {len(sample_data.get('results', []))} sample datasets\")\n",
    "    \n",
    "    # Extract metadata from first few datasets\n",
    "    sample_metadata = []\n",
    "    for dataset in sample_data.get('results', [])[:5]:\n",
    "        metadata = extractor.extract_dataset_metadata(dataset)\n",
    "        sample_metadata.append(metadata)\n",
    "    \n",
    "    sample_df = pd.DataFrame(sample_metadata)\n",
    "    print(\"\\nSample dataset metadata:\")\n",
    "    print(sample_df[['name', 'type', 'columns_count', 'download_count', 'page_views_total']].head())\n",
    "else:\n",
    "    print(f\"Error retrieving sample data: {sample_data['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b419e",
   "metadata": {},
   "source": [
    "## 4. Search and Filter Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78012ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoutDatasetSearcher:\n",
    "    \"\"\"\n",
    "    Advanced search and filtering capabilities for datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, extractor: ScoutDatasetExtractor):\n",
    "        self.extractor = extractor\n",
    "        self.socrata_search_url = \"http://api.us.socrata.com/api/catalog/v1\"\n",
    "    \n",
    "    def search_datasets_by_keyword(self, keyword: str, limit: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Search datasets by keyword using Socrata Discovery API\"\"\"\n",
    "        params = {\n",
    "            'domains': 'data.cityofnewyork.us',\n",
    "            'search_context': 'data.cityofnewyork.us',\n",
    "            'q': keyword,\n",
    "            'limit': limit\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.extractor.session.get(self.socrata_search_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            datasets = []\n",
    "            for dataset in data.get('results', []):\n",
    "                metadata = self.extractor.extract_dataset_metadata(dataset)\n",
    "                datasets.append(metadata)\n",
    "            \n",
    "            return pd.DataFrame(datasets)\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Search error: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def filter_by_category(self, datasets_df: pd.DataFrame, category: str) -> pd.DataFrame:\n",
    "        \"\"\"Filter datasets by domain category\"\"\"\n",
    "        return datasets_df[datasets_df['domain_category'].str.contains(category, case=False, na=False)]\n",
    "    \n",
    "    def filter_by_popularity(self, datasets_df: pd.DataFrame, min_downloads: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Filter datasets by download count\"\"\"\n",
    "        return datasets_df[datasets_df['download_count'] >= min_downloads]\n",
    "    \n",
    "    def filter_by_recency(self, datasets_df: pd.DataFrame, days_ago: int = 365) -> pd.DataFrame:\n",
    "        \"\"\"Filter datasets updated within specified days\"\"\"\n",
    "        if datasets_df.empty:\n",
    "            return datasets_df\n",
    "            \n",
    "        datasets_df = datasets_df.copy()\n",
    "        datasets_df['updatedAt'] = pd.to_datetime(datasets_df['updatedAt'], errors='coerce')\n",
    "        cutoff_date = datetime.now() - timedelta(days=days_ago)\n",
    "        \n",
    "        return datasets_df[datasets_df['updatedAt'] >= cutoff_date]\n",
    "    \n",
    "    def find_datasets_with_columns(self, datasets_df: pd.DataFrame, column_keywords: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Find datasets containing specific column types\"\"\"\n",
    "        def has_matching_columns(row):\n",
    "            if not row['columns_names']:\n",
    "                return False\n",
    "            \n",
    "            all_columns = ' '.join(row['columns_names']).lower()\n",
    "            return any(keyword.lower() in all_columns for keyword in column_keywords)\n",
    "        \n",
    "        return datasets_df[datasets_df.apply(has_matching_columns, axis=1)]\n",
    "    \n",
    "    def get_dataset_recommendations(self, dataset_id: str, datasets_df: pd.DataFrame, top_n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Simple recommendation based on similar tags and categories\"\"\"\n",
    "        if datasets_df.empty:\n",
    "            return datasets_df\n",
    "            \n",
    "        target_dataset = datasets_df[datasets_df['id'] == dataset_id]\n",
    "        if target_dataset.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        target_tags = set(target_dataset.iloc[0]['domain_tags'])\n",
    "        target_category = target_dataset.iloc[0]['domain_category']\n",
    "        \n",
    "        def similarity_score(row):\n",
    "            if row['id'] == dataset_id:\n",
    "                return 0\n",
    "            \n",
    "            score = 0\n",
    "            # Category match\n",
    "            if row['domain_category'] == target_category:\n",
    "                score += 3\n",
    "            \n",
    "            # Tag similarity\n",
    "            row_tags = set(row['domain_tags'])\n",
    "            common_tags = target_tags.intersection(row_tags)\n",
    "            score += len(common_tags)\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        datasets_df = datasets_df.copy()\n",
    "        datasets_df['similarity_score'] = datasets_df.apply(similarity_score, axis=1)\n",
    "        \n",
    "        return datasets_df[datasets_df['similarity_score'] > 0].nlargest(top_n, 'similarity_score')\n",
    "\n",
    "# Initialize searcher and demonstrate capabilities\n",
    "searcher = ScoutDatasetSearcher(extractor)\n",
    "\n",
    "print(\"=== DATASET SEARCH DEMONSTRATIONS ===\")\n",
    "\n",
    "# 1. Search by keyword\n",
    "print(\"\\n1. Searching for 'traffic' datasets:\")\n",
    "traffic_datasets = searcher.search_datasets_by_keyword(\"traffic\", limit=10)\n",
    "if not traffic_datasets.empty:\n",
    "    print(f\"Found {len(traffic_datasets)} traffic-related datasets\")\n",
    "    print(traffic_datasets[['name', 'domain_category', 'download_count']].head())\n",
    "else:\n",
    "    print(\"No traffic datasets found\")\n",
    "\n",
    "# 2. Search by keyword - crime\n",
    "print(\"\\n2. Searching for 'crime' datasets:\")\n",
    "crime_datasets = searcher.search_datasets_by_keyword(\"crime\", limit=10)\n",
    "if not crime_datasets.empty:\n",
    "    print(f\"Found {len(crime_datasets)} crime-related datasets\")\n",
    "    print(crime_datasets[['name', 'download_count', 'page_views_total']].head())\n",
    "\n",
    "# 3. Search by keyword - housing\n",
    "print(\"\\n3. Searching for 'housing' datasets:\")\n",
    "housing_datasets = searcher.search_datasets_by_keyword(\"housing\", limit=15)\n",
    "if not housing_datasets.empty:\n",
    "    print(f\"Found {len(housing_datasets)} housing-related datasets\")\n",
    "    popular_housing = searcher.filter_by_popularity(housing_datasets, min_downloads=500)\n",
    "    print(f\"Popular housing datasets (500+ downloads): {len(popular_housing)}\")\n",
    "    if not popular_housing.empty:\n",
    "        print(popular_housing[['name', 'download_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02f7db",
   "metadata": {},
   "source": [
    "## 5. Download Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee27cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoutDatasetDownloader:\n",
    "    \"\"\"\n",
    "    Download and sample datasets discovered through Scout/Socrata\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.base_url = \"https://data.cityofnewyork.us/resource\"\n",
    "    \n",
    "    def download_dataset_sample(self, dataset_id: str, limit: int = 1000, format: str = 'json') -> pd.DataFrame:\n",
    "        \"\"\"Download a sample of the dataset\"\"\"\n",
    "        url = f\"{self.base_url}/{dataset_id}.{format}\"\n",
    "        params = {'$limit': limit}\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if format == 'json':\n",
    "                data = response.json()\n",
    "                return pd.DataFrame(data)\n",
    "            elif format == 'csv':\n",
    "                return pd.read_csv(url, nrows=limit)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {format}\")\n",
    "                \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading {dataset_id}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_id}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_dataset_schema(self, dataset_id: str) -> Dict:\n",
    "        \"\"\"Get detailed schema information for a dataset\"\"\"\n",
    "        url = f\"https://data.cityofnewyork.us/api/views/{dataset_id}.json\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            schema_info = {\n",
    "                'id': data.get('id'),\n",
    "                'name': data.get('name'),\n",
    "                'description': data.get('description'),\n",
    "                'row_count': data.get('rowsUpdatedAt'),\n",
    "                'columns': []\n",
    "            }\n",
    "            \n",
    "            for col in data.get('columns', []):\n",
    "                col_info = {\n",
    "                    'id': col.get('id'),\n",
    "                    'name': col.get('name'),\n",
    "                    'field_name': col.get('fieldName'),\n",
    "                    'data_type': col.get('dataTypeName'),\n",
    "                    'description': col.get('description'),\n",
    "                    'render_type': col.get('renderTypeName')\n",
    "                }\n",
    "                schema_info['columns'].append(col_info)\n",
    "            \n",
    "            return schema_info\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def batch_download_samples(self, dataset_ids: List[str], sample_size: int = 100) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Download samples from multiple datasets\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for dataset_id in dataset_ids:\n",
    "            print(f\"Downloading sample from {dataset_id}...\")\n",
    "            sample_data = self.download_dataset_sample(dataset_id, limit=sample_size)\n",
    "            \n",
    "            if not sample_data.empty:\n",
    "                results[dataset_id] = sample_data\n",
    "                print(f\"  âœ“ Downloaded {len(sample_data)} rows, {len(sample_data.columns)} columns\")\n",
    "            else:\n",
    "                print(f\"  âœ— Failed to download {dataset_id}\")\n",
    "            \n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize downloader and demonstrate capabilities\n",
    "downloader = ScoutDatasetDownloader()\n",
    "\n",
    "print(\"=== DATASET DOWNLOAD DEMONSTRATIONS ===\")\n",
    "\n",
    "# Get some dataset IDs from our previous searches\n",
    "sample_dataset_ids = []\n",
    "\n",
    "# If we have traffic datasets, get their IDs\n",
    "if 'traffic_datasets' in locals() and not traffic_datasets.empty:\n",
    "    sample_dataset_ids.extend(traffic_datasets['id'].tolist()[:3])\n",
    "\n",
    "# If we have crime datasets, get their IDs  \n",
    "if 'crime_datasets' in locals() and not crime_datasets.empty:\n",
    "    sample_dataset_ids.extend(crime_datasets['id'].tolist()[:2])\n",
    "\n",
    "# Fallback to known NYC dataset IDs\n",
    "if not sample_dataset_ids:\n",
    "    sample_dataset_ids = [\n",
    "        'erm2-nwe9',  # 311 Service Requests\n",
    "        'h9gi-nx95',  # Motor Vehicle Collisions\n",
    "        'qgea-i56i'   # NYPD Complaint Data\n",
    "    ]\n",
    "\n",
    "print(f\"Attempting to download samples from {len(sample_dataset_ids)} datasets:\")\n",
    "print(sample_dataset_ids[:5])  # Show first 5 IDs\n",
    "\n",
    "# Download a schema example\n",
    "if sample_dataset_ids:\n",
    "    print(f\"\\n=== SCHEMA EXPLORATION FOR {sample_dataset_ids[0]} ===\")\n",
    "    schema = downloader.get_dataset_schema(sample_dataset_ids[0])\n",
    "    \n",
    "    if 'error' not in schema:\n",
    "        print(f\"Dataset: {schema.get('name')}\")\n",
    "        print(f\"Columns: {len(schema.get('columns', []))}\")\n",
    "        \n",
    "        # Show first few columns\n",
    "        for col in schema.get('columns', [])[:5]:\n",
    "            print(f\"  - {col['name']} ({col['data_type']}): {col['field_name']}\")\n",
    "    else:\n",
    "        print(f\"Schema error: {schema['error']}\")\n",
    "\n",
    "# Download actual sample data\n",
    "print(f\"\\n=== DOWNLOADING SAMPLE DATA ===\")\n",
    "sample_downloads = downloader.batch_download_samples(sample_dataset_ids[:2], sample_size=50)\n",
    "\n",
    "# Display info about downloaded samples\n",
    "for dataset_id, df in sample_downloads.items():\n",
    "    print(f\"\\nDataset {dataset_id}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)[:5]}...\")  # First 5 columns\n",
    "    if not df.empty:\n",
    "        print(f\"  Sample row:\\n{df.iloc[0].head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d658e00",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoutDataQualityAssessor:\n",
    "    \"\"\"\n",
    "    Assess data quality of datasets discovered through Scout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.assessment_results = {}\n",
    "    \n",
    "    def assess_dataset_quality(self, dataset_id: str, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "        if df.empty:\n",
    "            return {'error': 'Empty dataset'}\n",
    "        \n",
    "        assessment = {\n",
    "            'dataset_id': dataset_id,\n",
    "            'basic_stats': self._get_basic_stats(df),\n",
    "            'missing_data': self._assess_missing_data(df),\n",
    "            'data_types': self._assess_data_types(df),\n",
    "            'duplicates': self._assess_duplicates(df),\n",
    "            'outliers': self._assess_outliers(df),\n",
    "            'completeness_score': 0,\n",
    "            'usability_score': 0\n",
    "        }\n",
    "        \n",
    "        # Calculate overall scores\n",
    "        assessment['completeness_score'] = self._calculate_completeness_score(assessment)\n",
    "        assessment['usability_score'] = self._calculate_usability_score(assessment)\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def _get_basic_stats(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Basic dataset statistics\"\"\"\n",
    "        return {\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024 * 1024),\n",
    "            'column_names': list(df.columns)\n",
    "        }\n",
    "    \n",
    "    def _assess_missing_data(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Assess missing data patterns\"\"\"\n",
    "        missing_counts = df.isnull().sum()\n",
    "        missing_percentages = (missing_counts / len(df)) * 100\n",
    "        \n",
    "        return {\n",
    "            'total_missing_values': missing_counts.sum(),\n",
    "            'missing_percentage_overall': (missing_counts.sum() / (len(df) * len(df.columns))) * 100,\n",
    "            'columns_with_missing': missing_counts[missing_counts > 0].to_dict(),\n",
    "            'missing_percentages_by_column': missing_percentages[missing_percentages > 0].to_dict(),\n",
    "            'completely_empty_columns': missing_percentages[missing_percentages == 100].index.tolist()\n",
    "        }\n",
    "    \n",
    "    def _assess_data_types(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Assess data type consistency and issues\"\"\"\n",
    "        type_summary = df.dtypes.value_counts().to_dict()\n",
    "        \n",
    "        # Check for potential data type issues\n",
    "        potential_dates = []\n",
    "        potential_numbers = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Check if text columns might be dates or numbers\n",
    "            if df[col].dtype == 'object':\n",
    "                sample_values = df[col].dropna().astype(str).head(10)\n",
    "                \n",
    "                # Check for date patterns\n",
    "                date_patterns = [r'\\d{4}-\\d{2}-\\d{2}', r'\\d{2}/\\d{2}/\\d{4}', r'\\d{2}-\\d{2}-\\d{4}']\n",
    "                for pattern in date_patterns:\n",
    "                    if sample_values.str.match(pattern).any():\n",
    "                        potential_dates.append(col)\n",
    "                        break\n",
    "                \n",
    "                # Check for number patterns\n",
    "                if sample_values.str.match(r'^[\\d.,]+$').any():\n",
    "                    potential_numbers.append(col)\n",
    "        \n",
    "        return {\n",
    "            'type_distribution': {str(k): v for k, v in type_summary.items()},\n",
    "            'potential_date_columns': potential_dates,\n",
    "            'potential_numeric_columns': potential_numbers\n",
    "        }\n",
    "    \n",
    "    def _assess_duplicates(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Assess duplicate records\"\"\"\n",
    "        total_duplicates = df.duplicated().sum()\n",
    "        \n",
    "        return {\n",
    "            'total_duplicate_rows': int(total_duplicates),\n",
    "            'duplicate_percentage': (total_duplicates / len(df)) * 100,\n",
    "            'unique_row_count': len(df.drop_duplicates())\n",
    "        }\n",
    "    \n",
    "    def _assess_outliers(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Assess outliers in numeric columns\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        outlier_info = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if not df[col].empty:\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "                \n",
    "                outlier_info[col] = {\n",
    "                    'outlier_count': len(outliers),\n",
    "                    'outlier_percentage': (len(outliers) / len(df)) * 100,\n",
    "                    'min_value': float(df[col].min()),\n",
    "                    'max_value': float(df[col].max()),\n",
    "                    'mean': float(df[col].mean()),\n",
    "                    'std': float(df[col].std())\n",
    "                }\n",
    "        \n",
    "        return outlier_info\n",
    "    \n",
    "    def _calculate_completeness_score(self, assessment: Dict) -> float:\n",
    "        \"\"\"Calculate a completeness score (0-100)\"\"\"\n",
    "        missing_data = assessment['missing_data']\n",
    "        \n",
    "        # Base score\n",
    "        score = 100\n",
    "        \n",
    "        # Penalize missing data\n",
    "        score -= missing_data['missing_percentage_overall']\n",
    "        \n",
    "        # Penalize completely empty columns\n",
    "        empty_cols_penalty = len(missing_data['completely_empty_columns']) * 10\n",
    "        score -= empty_cols_penalty\n",
    "        \n",
    "        return max(0, min(100, score))\n",
    "    \n",
    "    def _calculate_usability_score(self, assessment: Dict) -> float:\n",
    "        \"\"\"Calculate a usability score (0-100)\"\"\"\n",
    "        score = 100\n",
    "        \n",
    "        # Penalize high duplicate percentage\n",
    "        dup_penalty = assessment['duplicates']['duplicate_percentage'] * 0.5\n",
    "        score -= dup_penalty\n",
    "        \n",
    "        # Reward proper data types\n",
    "        basic_stats = assessment['basic_stats']\n",
    "        if basic_stats['column_count'] > 0:\n",
    "            # Bonus for having diverse data types\n",
    "            type_diversity = len(assessment['data_types']['type_distribution'])\n",
    "            score += min(10, type_diversity * 2)\n",
    "        \n",
    "        return max(0, min(100, score))\n",
    "    \n",
    "    def generate_quality_report(self, assessments: Dict[str, Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Generate a summary quality report\"\"\"\n",
    "        reports = []\n",
    "        \n",
    "        for dataset_id, assessment in assessments.items():\n",
    "            if 'error' in assessment:\n",
    "                continue\n",
    "                \n",
    "            report = {\n",
    "                'dataset_id': dataset_id,\n",
    "                'row_count': assessment['basic_stats']['row_count'],\n",
    "                'column_count': assessment['basic_stats']['column_count'],\n",
    "                'missing_percentage': round(assessment['missing_data']['missing_percentage_overall'], 2),\n",
    "                'duplicate_percentage': round(assessment['duplicates']['duplicate_percentage'], 2),\n",
    "                'completeness_score': round(assessment['completeness_score'], 2),\n",
    "                'usability_score': round(assessment['usability_score'], 2),\n",
    "                'overall_quality': round((assessment['completeness_score'] + assessment['usability_score']) / 2, 2)\n",
    "            }\n",
    "            reports.append(report)\n",
    "        \n",
    "        return pd.DataFrame(reports)\n",
    "\n",
    "# Initialize quality assessor and run assessments\n",
    "quality_assessor = ScoutDataQualityAssessor()\n",
    "\n",
    "print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "# Assess quality of downloaded samples\n",
    "quality_assessments = {}\n",
    "\n",
    "if 'sample_downloads' in locals():\n",
    "    for dataset_id, df in sample_downloads.items():\n",
    "        print(f\"\\nAssessing quality of {dataset_id}...\")\n",
    "        assessment = quality_assessor.assess_dataset_quality(dataset_id, df)\n",
    "        quality_assessments[dataset_id] = assessment\n",
    "        \n",
    "        if 'error' not in assessment:\n",
    "            print(f\"  Completeness Score: {assessment['completeness_score']:.1f}/100\")\n",
    "            print(f\"  Usability Score: {assessment['usability_score']:.1f}/100\")\n",
    "            print(f\"  Missing Data: {assessment['missing_data']['missing_percentage_overall']:.1f}%\")\n",
    "            print(f\"  Duplicates: {assessment['duplicates']['duplicate_percentage']:.1f}%\")\n",
    "        else:\n",
    "            print(f\"  Assessment Error: {assessment['error']}\")\n",
    "\n",
    "# Generate quality report\n",
    "if quality_assessments:\n",
    "    print(\"\\n=== QUALITY SUMMARY REPORT ===\")\n",
    "    quality_report = quality_assessor.generate_quality_report(quality_assessments)\n",
    "    print(quality_report)\n",
    "    \n",
    "    # Visualize quality scores\n",
    "    if len(quality_report) > 1:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(range(len(quality_report)), quality_report['completeness_score'])\n",
    "        plt.title('Dataset Completeness Scores')\n",
    "        plt.xlabel('Dataset Index')\n",
    "        plt.ylabel('Completeness Score')\n",
    "        plt.xticks(range(len(quality_report)), quality_report['dataset_id'], rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(range(len(quality_report)), quality_report['usability_score'])\n",
    "        plt.title('Dataset Usability Scores')\n",
    "        plt.xlabel('Dataset Index')\n",
    "        plt.ylabel('Usability Score')\n",
    "        plt.xticks(range(len(quality_report)), quality_report['dataset_id'], rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No datasets available for quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b9cee",
   "metadata": {},
   "source": [
    "## 7. Automated Data Discovery Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ba765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoutAutomatedPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive automated data discovery and assessment pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extractor = ScoutDatasetExtractor()\n",
    "        self.searcher = ScoutDatasetSearcher(self.extractor)\n",
    "        self.downloader = ScoutDatasetDownloader()\n",
    "        self.quality_assessor = ScoutDataQualityAssessor()\n",
    "        \n",
    "        self.pipeline_results = {\n",
    "            'datasets_metadata': pd.DataFrame(),\n",
    "            'quality_assessments': {},\n",
    "            'recommendations': {},\n",
    "            'search_results': {},\n",
    "            'pipeline_stats': {}\n",
    "        }\n",
    "    \n",
    "    def run_discovery_pipeline(self, \n",
    "                              search_terms: List[str] = None,\n",
    "                              max_datasets_per_term: int = 20,\n",
    "                              quality_sample_size: int = 100,\n",
    "                              include_recommendations: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Run complete automated discovery pipeline\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"ðŸš€ Starting Scout Automated Data Discovery Pipeline\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Step 1: Search for datasets based on terms\n",
    "        all_datasets = pd.DataFrame()\n",
    "        \n",
    "        if search_terms:\n",
    "            print(f\"\\nðŸ“‹ Step 1: Searching for datasets with terms: {search_terms}\")\n",
    "            for term in search_terms:\n",
    "                print(f\"  Searching for '{term}'...\")\n",
    "                search_results = self.searcher.search_datasets_by_keyword(term, limit=max_datasets_per_term)\n",
    "                \n",
    "                if not search_results.empty:\n",
    "                    search_results['search_term'] = term\n",
    "                    all_datasets = pd.concat([all_datasets, search_results], ignore_index=True)\n",
    "                    self.pipeline_results['search_results'][term] = search_results\n",
    "                    print(f\"    Found {len(search_results)} datasets for '{term}'\")\n",
    "                else:\n",
    "                    print(f\"    No datasets found for '{term}'\")\n",
    "        else:\n",
    "            print(f\"\\nðŸ“‹ Step 1: Getting general dataset sample...\")\n",
    "            all_datasets = self.extractor.get_comprehensive_dataset_list(max_datasets=50)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        if not all_datasets.empty:\n",
    "            initial_count = len(all_datasets)\n",
    "            all_datasets = all_datasets.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "            print(f\"  Removed {initial_count - len(all_datasets)} duplicate datasets\")\n",
    "            self.pipeline_results['datasets_metadata'] = all_datasets\n",
    "        \n",
    "        if all_datasets.empty:\n",
    "            print(\"âŒ No datasets found. Pipeline terminated.\")\n",
    "            return self.pipeline_results\n",
    "        \n",
    "        print(f\"  Total unique datasets found: {len(all_datasets)}\\\")\\n\\\")\\n        \\n        # Step 2: Download samples and assess quality\\n        print(f\\\"ðŸ” Step 2: Quality Assessment (sampling {quality_sample_size} rows per dataset)\\\")\\n        \\n        # Select top datasets for quality assessment\\n        assessment_datasets = self._select_datasets_for_assessment(all_datasets, max_count=10)\\n        \\n        quality_results = {}\\n        for _, dataset in assessment_datasets.iterrows():\\n            dataset_id = dataset['id']\\n            dataset_name = dataset['name']\\n            \\n            print(f\\\"  Assessing {dataset_id}: {dataset_name[:50]}...\\\")\\n            \\n            # Download sample\\n            sample_df = self.downloader.download_dataset_sample(dataset_id, limit=quality_sample_size)\\n            \\n            if not sample_df.empty:\\n                # Assess quality\\n                quality_assessment = self.quality_assessor.assess_dataset_quality(dataset_id, sample_df)\\n                quality_results[dataset_id] = quality_assessment\\n                \\n                if 'error' not in quality_assessment:\\n                    print(f\\\"    âœ“ Quality scores - Completeness: {quality_assessment['completeness_score']:.1f}, Usability: {quality_assessment['usability_score']:.1f}\\\")\\n                else:\\n                    print(f\\\"    âœ— Assessment failed: {quality_assessment['error']}\\\")\\n            else:\\n                print(f\\\"    âœ— Failed to download sample\\\")\\n                \\n            time.sleep(0.5)  # Rate limiting\\n        \\n        self.pipeline_results['quality_assessments'] = quality_results\\n        \\n        # Step 3: Generate recommendations\\n        if include_recommendations and len(quality_results) > 1:\\n            print(f\\\"\\\\nðŸŽ¯ Step 3: Generating Dataset Recommendations\\\")\\n            \\n            for dataset_id in list(quality_results.keys())[:3]:  # Top 3 for recommendations\\n                recommendations = self.searcher.get_dataset_recommendations(\\n                    dataset_id, all_datasets, top_n=5\\n                )\\n                self.pipeline_results['recommendations'][dataset_id] = recommendations\\n                print(f\\\"  Generated {len(recommendations)} recommendations for {dataset_id}\\\")\\n        \\n        # Step 4: Generate pipeline statistics\\n        end_time = datetime.now()\\n        duration = (end_time - start_time).total_seconds()\\n        \\n        self.pipeline_results['pipeline_stats'] = {\\n            'total_datasets_found': len(all_datasets),\\n            'datasets_assessed': len(quality_results),\\n            'search_terms_used': search_terms or [],\\n            'execution_time_seconds': duration,\\n            'average_completeness_score': np.mean([a['completeness_score'] for a in quality_results.values() if 'completeness_score' in a]),\\n            'average_usability_score': np.mean([a['usability_score'] for a in quality_results.values() if 'usability_score' in a]),\\n            'pipeline_timestamp': start_time.isoformat()\\n        }\\n        \\n        print(f\\\"\\\\nâœ… Pipeline completed in {duration:.1f} seconds\\\")\\n        self._print_pipeline_summary()\\n        \\n        return self.pipeline_results\\n    \\n    def _select_datasets_for_assessment(self, datasets_df: pd.DataFrame, max_count: int = 10) -> pd.DataFrame:\\n        \\\"\\\"\\\"Select most promising datasets for quality assessment\\\"\\\"\\\"\\n        if len(datasets_df) <= max_count:\\n            return datasets_df\\n        \\n        # Prioritize by download count and page views\\n        scored_datasets = datasets_df.copy()\\n        scored_datasets['priority_score'] = (\\n            scored_datasets['download_count'].fillna(0) * 0.7 +\\n            scored_datasets['page_views_total'].fillna(0) * 0.3\\n        )\\n        \\n        return scored_datasets.nlargest(max_count, 'priority_score')\\n    \\n    def _print_pipeline_summary(self):\\n        \\\"\\\"\\\"Print a summary of pipeline results\\\"\\\"\\\"\\n        stats = self.pipeline_results['pipeline_stats']\\n        \\n        print(f\\\"\\\\nðŸ“Š PIPELINE SUMMARY\\\")\\n        print(f\\\"  Datasets Discovered: {stats['total_datasets_found']}\\\")\\n        print(f\\\"  Datasets Assessed: {stats['datasets_assessed']}\\\")\\n        print(f\\\"  Execution Time: {stats['execution_time_seconds']:.1f} seconds\\\")\\n        \\n        if stats.get('average_completeness_score'):\\n            print(f\\\"  Average Completeness: {stats['average_completeness_score']:.1f}/100\\\")\\n            print(f\\\"  Average Usability: {stats['average_usability_score']:.1f}/100\\\")\\n        \\n        # Show top quality datasets\\n        quality_assessments = self.pipeline_results['quality_assessments']\\n        if quality_assessments:\\n            print(f\\\"\\\\nðŸ† TOP QUALITY DATASETS:\\\")\\n            \\n            # Sort by overall quality\\n            sorted_quality = sorted(\\n                quality_assessments.items(), \\n                key=lambda x: (x[1].get('completeness_score', 0) + x[1].get('usability_score', 0)) / 2,\\n                reverse=True\\n            )\\n            \\n            for i, (dataset_id, assessment) in enumerate(sorted_quality[:3]):\\n                if 'error' not in assessment:\\n                    overall_score = (assessment['completeness_score'] + assessment['usability_score']) / 2\\n                    dataset_name = next(\\n                        (row['name'] for _, row in self.pipeline_results['datasets_metadata'].iterrows() \\n                         if row['id'] == dataset_id), \\n                        'Unknown'\\n                    )\\n                    print(f\\\"  {i+1}. {dataset_id}: {dataset_name[:40]}... (Score: {overall_score:.1f})\\\")\\n\\n# Initialize and run the automated pipeline\\npipeline = ScoutAutomatedPipeline()\\n\\nprint(\\\"=== AUTOMATED SCOUT DATA DISCOVERY PIPELINE ===\\\")\\n\\n# Define search terms for targeted discovery\\nsearch_terms = ['transportation', 'housing', 'health', 'education']\\n\\n# Run the complete pipeline\\nresults = pipeline.run_discovery_pipeline(\\n    search_terms=search_terms,\\n    max_datasets_per_term=15,\\n    quality_sample_size=50,\\n    include_recommendations=True\\n)\\n\\n# Display additional insights\\nif results['datasets_metadata'] is not None and not results['datasets_metadata'].empty:\\n    print(f\\\"\\\\nðŸ“ˆ DATASET INSIGHTS:\\\")\\n    \\n    metadata_df = results['datasets_metadata']\\n    \\n    # Category distribution\\n    if 'domain_category' in metadata_df.columns:\\n        category_counts = metadata_df['domain_category'].value_counts().head(5)\\n        print(f\\\"\\\\n  Top Categories:\\\")\\n        for category, count in category_counts.items():\\n            print(f\\\"    {category}: {count} datasets\\\")\\n    \\n    # Most popular datasets\\n    if 'download_count' in metadata_df.columns:\\n        popular_datasets = metadata_df.nlargest(3, 'download_count')\\n        print(f\\\"\\\\n  Most Downloaded:\\\")\\n        for _, dataset in popular_datasets.iterrows():\\n            print(f\\\"    {dataset['name'][:50]}... ({dataset['download_count']} downloads)\\\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bec1c",
   "metadata": {},
   "source": [
    "## 8. Export Findings and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoutDataExporter:\n",
    "    \"\"\"\n",
    "    Export Scout discovery results to various formats\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_results: Dict):\n",
    "        self.results = pipeline_results\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    def export_dataset_catalog(self, filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Export comprehensive dataset catalog\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"scout_dataset_catalog_{self.timestamp}.csv\"\n",
    "        \n",
    "        if self.results['datasets_metadata'].empty:\n",
    "            print(\"No dataset metadata to export\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Prepare export data\n",
    "        export_df = self.results['datasets_metadata'].copy()\n",
    "        \n",
    "        # Add quality scores if available\n",
    "        quality_assessments = self.results['quality_assessments']\n",
    "        if quality_assessments:\n",
    "            quality_data = []\n",
    "            for _, row in export_df.iterrows():\n",
    "                dataset_id = row['id']\n",
    "                if dataset_id in quality_assessments:\n",
    "                    assessment = quality_assessments[dataset_id]\n",
    "                    if 'error' not in assessment:\n",
    "                        quality_data.append({\n",
    "                            'id': dataset_id,\n",
    "                            'completeness_score': assessment['completeness_score'],\n",
    "                            'usability_score': assessment['usability_score'],\n",
    "                            'missing_percentage': assessment['missing_data']['missing_percentage_overall'],\n",
    "                            'duplicate_percentage': assessment['duplicates']['duplicate_percentage']\n",
    "                        })\n",
    "            \n",
    "            if quality_data:\n",
    "                quality_df = pd.DataFrame(quality_data)\n",
    "                export_df = export_df.merge(quality_df, on='id', how='left')\n",
    "        \n",
    "        # Export to CSV\n",
    "        export_df.to_csv(filename, index=False)\n",
    "        print(f\"âœ… Dataset catalog exported to: {filename}\")\n",
    "        print(f\"   Records: {len(export_df)}\")\n",
    "        print(f\"   Columns: {len(export_df.columns)}\")\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def export_quality_report(self, filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Export detailed quality assessment report\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"scout_quality_report_{self.timestamp}.json\"\n",
    "        \n",
    "        quality_assessments = self.results['quality_assessments']\n",
    "        if not quality_assessments:\n",
    "            print(\"No quality assessments to export\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Prepare quality report\n",
    "        quality_report = {\n",
    "            'metadata': {\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'total_datasets_assessed': len(quality_assessments),\n",
    "                'pipeline_stats': self.results.get('pipeline_stats', {})\n",
    "            },\n",
    "            'assessments': quality_assessments,\n",
    "            'summary_statistics': self._calculate_quality_summary()\n",
    "        }\n",
    "        \n",
    "        # Export to JSON\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(quality_report, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"âœ… Quality report exported to: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def export_recommendations(self, filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Export dataset recommendations\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"scout_recommendations_{self.timestamp}.json\"\n",
    "        \n",
    "        recommendations = self.results['recommendations']\n",
    "        if not recommendations:\n",
    "            print(\"No recommendations to export\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Prepare recommendations export\n",
    "        recommendations_export = {\n",
    "            'metadata': {\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'total_source_datasets': len(recommendations)\n",
    "            },\n",
    "            'recommendations': {}\n",
    "        }\n",
    "        \n",
    "        for dataset_id, rec_df in recommendations.items():\n",
    "            if not rec_df.empty:\n",
    "                recommendations_export['recommendations'][dataset_id] = {\n",
    "                    'source_dataset': dataset_id,\n",
    "                    'recommended_datasets': rec_df[['id', 'name', 'similarity_score', 'domain_category']].to_dict('records')\n",
    "                }\n",
    "        \n",
    "        # Export to JSON\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(recommendations_export, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"âœ… Recommendations exported to: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def export_search_results(self, filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Export search results by term\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"scout_search_results_{self.timestamp}.json\"\n",
    "        \n",
    "        search_results = self.results['search_results']\n",
    "        if not search_results:\n",
    "            print(\"No search results to export\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Prepare search results export\n",
    "        search_export = {\n",
    "            'metadata': {\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'search_terms': list(search_results.keys())\n",
    "            },\n",
    "            'results_by_term': {}\n",
    "        }\n",
    "        \n",
    "        for term, results_df in search_results.items():\n",
    "            if not results_df.empty:\n",
    "                search_export['results_by_term'][term] = {\n",
    "                    'term': term,\n",
    "                    'total_results': len(results_df),\n",
    "                    'datasets': results_df[['id', 'name', 'domain_category', 'download_count']].to_dict('records')\n",
    "                }\n",
    "        \n",
    "        # Export to JSON\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(search_export, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"âœ… Search results exported to: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def export_complete_package(self, base_filename: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Export all findings in a complete package\"\"\"\n",
    "        if base_filename is None:\n",
    "            base_filename = f\"scout_discovery_{self.timestamp}\"\n",
    "        \n",
    "        exported_files = []\n",
    "        \n",
    "        # Export all components\n",
    "        catalog_file = self.export_dataset_catalog(f\"{base_filename}_catalog.csv\")\n",
    "        if catalog_file:\n",
    "            exported_files.append(catalog_file)\n",
    "        \n",
    "        quality_file = self.export_quality_report(f\"{base_filename}_quality.json\")\n",
    "        if quality_file:\n",
    "            exported_files.append(quality_file)\n",
    "        \n",
    "        recommendations_file = self.export_recommendations(f\"{base_filename}_recommendations.json\")\n",
    "        if recommendations_file:\n",
    "            exported_files.append(recommendations_file)\n",
    "        \n",
    "        search_file = self.export_search_results(f\"{base_filename}_search.json\")\n",
    "        if search_file:\n",
    "            exported_files.append(search_file)\n",
    "        \n",
    "        # Create summary file\n",
    "        summary_file = f\"{base_filename}_summary.txt\"\n",
    "        self._export_summary_report(summary_file)\n",
    "        exported_files.append(summary_file)\n",
    "        \n",
    "        print(f\"\\nðŸ“¦ Complete package exported: {len(exported_files)} files\")\n",
    "        for file in exported_files:\n",
    "            print(f\"   - {file}\")\n",
    "        \n",
    "        return exported_files\n",
    "    \n",
    "    def _calculate_quality_summary(self) -> Dict:\n",
    "        \"\"\"Calculate summary statistics for quality assessments\"\"\"\n",
    "        quality_assessments = self.results['quality_assessments']\n",
    "        \n",
    "        if not quality_assessments:\n",
    "            return {}\n",
    "        \n",
    "        valid_assessments = [a for a in quality_assessments.values() if 'error' not in a]\n",
    "        \n",
    "        if not valid_assessments:\n",
    "            return {}\n",
    "        \n",
    "        completeness_scores = [a['completeness_score'] for a in valid_assessments]\n",
    "        usability_scores = [a['usability_score'] for a in valid_assessments]\n",
    "        missing_percentages = [a['missing_data']['missing_percentage_overall'] for a in valid_assessments]\n",
    "        \n",
    "        return {\n",
    "            'total_assessed': len(valid_assessments),\n",
    "            'completeness': {\n",
    "                'mean': np.mean(completeness_scores),\n",
    "                'median': np.median(completeness_scores),\n",
    "                'min': np.min(completeness_scores),\n",
    "                'max': np.max(completeness_scores)\n",
    "            },\n",
    "            'usability': {\n",
    "                'mean': np.mean(usability_scores),\n",
    "                'median': np.median(usability_scores),\n",
    "                'min': np.min(usability_scores),\n",
    "                'max': np.max(usability_scores)\n",
    "            },\n",
    "            'missing_data': {\n",
    "                'mean_percentage': np.mean(missing_percentages),\n",
    "                'median_percentage': np.median(missing_percentages)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _export_summary_report(self, filename: str):\n",
    "        \"\"\"Export a human-readable summary report\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"SCOUT DATA DISCOVERY SUMMARY REPORT\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            # Pipeline stats\n",
    "            stats = self.results.get('pipeline_stats', {})\n",
    "            f.write(f\"Generated: {stats.get('pipeline_timestamp', 'Unknown')}\\n\")\n",
    "            f.write(f\"Execution Time: {stats.get('execution_time_seconds', 0):.1f} seconds\\n\")\n",
    "            f.write(f\"Search Terms: {', '.join(stats.get('search_terms_used', []))}\\n\")\n",
    "            f.write(f\"Datasets Found: {stats.get('total_datasets_found', 0)}\\n\")\n",
    "            f.write(f\"Datasets Assessed: {stats.get('datasets_assessed', 0)}\\n\\n\")\n",
    "            \n",
    "            # Quality summary\n",
    "            quality_summary = self._calculate_quality_summary()\n",
    "            if quality_summary:\n",
    "                f.write(\"QUALITY SUMMARY\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\")\n",
    "                f.write(f\"Average Completeness Score: {quality_summary['completeness']['mean']:.1f}/100\\n\")\n",
    "                f.write(f\"Average Usability Score: {quality_summary['usability']['mean']:.1f}/100\\n\")\n",
    "                f.write(f\"Average Missing Data: {quality_summary['missing_data']['mean_percentage']:.1f}%\\n\\n\")\n",
    "            \n",
    "            # Top datasets\n",
    "            if not self.results['datasets_metadata'].empty:\n",
    "                f.write(\"TOP DATASETS BY DOWNLOADS\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                top_datasets = self.results['datasets_metadata'].nlargest(5, 'download_count')\n",
    "                for i, (_, dataset) in enumerate(top_datasets.iterrows(), 1):\n",
    "                    f.write(f\"{i}. {dataset['name']} ({dataset['download_count']} downloads)\\n\")\n",
    "        \n",
    "        print(f\"âœ… Summary report exported to: {filename}\")\n",
    "\n",
    "# Export all findings from the pipeline\n",
    "if 'results' in locals() and results:\n",
    "    print(\"\\nðŸ“ EXPORTING SCOUT DISCOVERY FINDINGS\")\n",
    "    \n",
    "    exporter = ScoutDataExporter(results)\n",
    "    \n",
    "    # Export complete package\n",
    "    exported_files = exporter.export_complete_package()\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Scout exploration complete! All findings exported to {len(exported_files)} files.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No results available for export. Run the pipeline first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
