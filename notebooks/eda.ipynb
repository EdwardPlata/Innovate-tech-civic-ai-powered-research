{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3267062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "Python version: 6\n",
      "Python path: /home/codespace/.python/current/bin/python3\n"
     ]
    }
   ],
   "source": [
    "# Simple test cell\n",
    "print(\"Hello World!\")\n",
    "print(f\"Python version: {3 + 3}\")\n",
    "import sys\n",
    "print(f\"Python path: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871eac20",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2790270973.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mThe Scout backend provides these key endpoints:\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9d5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064883b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scout backend API is accessible!\n",
      "Status: healthy\n"
     ]
    }
   ],
   "source": [
    "# Scout Backend API Configuration\n",
    "BASE_URL = \"http://localhost:8080/api\"\n",
    "\n",
    "def make_api_request(endpoint, method=\"GET\", data=None):\n",
    "    \"\"\"Helper function to make API requests to Scout backend\"\"\"\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    \n",
    "    try:\n",
    "        if method == \"POST\":\n",
    "            response = requests.post(url, json=data, timeout=30)\n",
    "        else:\n",
    "            response = requests.get(url, timeout=30)\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå API request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test API connection\n",
    "health_check = make_api_request(\"health\")\n",
    "if health_check:\n",
    "    print(\"‚úÖ Scout backend API is accessible!\")\n",
    "    print(f\"Status: {health_check.get('status', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot connect to Scout backend API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a132fd0",
   "metadata": {},
   "source": [
    "## Step 1: Search for Automobile Accident Datasets\n",
    "\n",
    "Let's search for automobile accident and collision datasets from NYC Open Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd510c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for automobile accident datasets...\n",
      "‚úÖ Found 40 datasets!\n",
      "\n",
      "üìä Dataset Overview:\n",
      "Total datasets found: 40\n",
      "Categories: {'Transportation': 26, 'Public Safety': 8, 'Housing & Development': 2, 'City Government': 2, 'Health': 1, 'Business': 1}\n",
      "\n",
      "üî• Top 5 Most Downloaded Datasets:\n",
      "1. Motor Vehicle Collisions - Crashes...\n",
      "   Downloads: 263,671 | Updated: 2025-09-16 | Category: Public Safety\n",
      "\n",
      "11. DOB Complaints Received...\n",
      "   Downloads: 52,089 | Updated: 2025-09-16 | Category: Housing & Development\n",
      "\n",
      "3. Motor Vehicle Collisions - Vehicles...\n",
      "   Downloads: 14,263 | Updated: 2025-09-16 | Category: Public Safety\n",
      "\n",
      "2. Motor Vehicle Collisions - Person...\n",
      "   Downloads: 10,899 | Updated: 2025-09-16 | Category: Public Safety\n",
      "\n",
      "28. DOT Traffic Speeds NBE...\n",
      "   Downloads: 9,201 | Updated: 2025-09-17 | Category: Transportation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for automobile accident datasets\n",
    "search_terms = [\"collision\", \"accident\", \"motor vehicle\", \"crash\", \"traffic\"]\n",
    "\n",
    "search_data = {\n",
    "    \"search_terms\": search_terms,\n",
    "    \"limit\": 20,\n",
    "    \"include_quality\": True\n",
    "}\n",
    "\n",
    "print(\"üîç Searching for automobile accident datasets...\")\n",
    "datasets = make_api_request(\"datasets/search\", method=\"POST\", data=search_data)\n",
    "\n",
    "if datasets:\n",
    "    print(f\"‚úÖ Found {len(datasets)} datasets!\")\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_datasets = pd.DataFrame(datasets)\n",
    "    \n",
    "    # Display key information\n",
    "    print(\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"Total datasets found: {len(df_datasets)}\")\n",
    "    print(f\"Categories: {df_datasets['category'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Show top datasets by download count\n",
    "    top_datasets = df_datasets.nlargest(5, 'download_count')[\n",
    "        ['name', 'download_count', 'updated_at', 'category']\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüî• Top 5 Most Downloaded Datasets:\")\n",
    "    for idx, row in top_datasets.iterrows():\n",
    "        print(f\"{idx+1}. {row['name'][:60]}...\")\n",
    "        print(f\"   Downloads: {row['download_count']:,} | Updated: {row['updated_at'][:10]} | Category: {row['category']}\")\n",
    "        print()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No datasets found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf08ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Most Recently Updated Automobile Accident Dataset:\n",
      "Name: DOT Traffic Speeds NBE\n",
      "ID: i4gi-tjb9\n",
      "Updated: 2025-09-17 00:41:45+00:00\n",
      "Downloads: 9,201\n",
      "Category: Transportation\n",
      "Description: No description available...\n",
      "\n",
      "‚úÖ Selected dataset: i4gi-tjb9\n"
     ]
    }
   ],
   "source": [
    "# Get the most recently updated dataset\n",
    "if datasets and len(df_datasets) > 0:\n",
    "    # Sort by updated_at to find most recent\n",
    "    df_datasets['updated_at'] = pd.to_datetime(df_datasets['updated_at'])\n",
    "    most_recent = df_datasets.loc[df_datasets['updated_at'].idxmax()]\n",
    "    \n",
    "    print(\"üïê Most Recently Updated Automobile Accident Dataset:\")\n",
    "    print(f\"Name: {most_recent['name']}\")\n",
    "    print(f\"ID: {most_recent['id']}\")\n",
    "    print(f\"Updated: {most_recent['updated_at']}\")\n",
    "    print(f\"Downloads: {most_recent['download_count']:,}\")\n",
    "    print(f\"Category: {most_recent['category']}\")\n",
    "    print(f\"Description: {most_recent['description'][:200]}...\")\n",
    "    \n",
    "    # Store the dataset ID for further analysis\n",
    "    target_dataset_id = most_recent['id']\n",
    "    target_dataset_name = most_recent['name']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Selected dataset: {target_dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d61d7e",
   "metadata": {},
   "source": [
    "## Step 2: Download and Analyze Dataset Sample\n",
    "\n",
    "Now let's get a sample of the most recent automobile accident dataset and perform initial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "598e9f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading sample data for: DOT Traffic Speeds NBE\n",
      "‚úÖ Sample data downloaded successfully!\n",
      "\n",
      "üìä Dataset Shape: (100, 13)\n",
      "Columns: 13\n",
      "Sample Size: 100 records\n",
      "\n",
      "üìã Column Information:\n",
      "Total Columns: 13\n",
      "Data Types: {dtype('O'): 13}\n",
      "\n",
      "First 10 Columns:\n",
      " 1. id\n",
      " 2. speed\n",
      " 3. travel_time\n",
      " 4. status\n",
      " 5. data_as_of\n",
      " 6. link_id\n",
      " 7. link_points\n",
      " 8. encoded_poly_line\n",
      " 9. encoded_poly_line_lvls\n",
      "10. owner\n",
      "... and 3 more columns\n"
     ]
    }
   ],
   "source": [
    "# Download dataset sample using Scout API\n",
    "if 'target_dataset_id' in locals():\n",
    "    print(f\"üì• Downloading sample data for: {target_dataset_name}\")\n",
    "    \n",
    "    # Get dataset sample (1000 records)\n",
    "    sample_data = make_api_request(f\"datasets/{target_dataset_id}/sample?sample_size=1000\")\n",
    "    \n",
    "    if sample_data and sample_data.get('data'):\n",
    "        print(\"‚úÖ Sample data downloaded successfully!\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(sample_data['data'])\n",
    "        \n",
    "        print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "        print(f\"Columns: {len(df.columns)}\")\n",
    "        print(f\"Sample Size: {len(df)} records\")\n",
    "        \n",
    "        # Display basic info\n",
    "        print(\"\\nüìã Column Information:\")\n",
    "        print(f\"Total Columns: {len(df.columns)}\")\n",
    "        print(f\"Data Types: {df.dtypes.value_counts().to_dict()}\")\n",
    "        \n",
    "        # Show first few column names\n",
    "        print(f\"\\nFirst 10 Columns:\")\n",
    "        for i, col in enumerate(df.columns[:10]):\n",
    "            print(f\"{i+1:2d}. {col}\")\n",
    "        \n",
    "        if len(df.columns) > 10:\n",
    "            print(f\"... and {len(df.columns) - 10} more columns\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå Failed to download sample data\")\n",
    "else:\n",
    "    print(\"‚ùå No target dataset selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ce44826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data structure and quality\n",
    "if 'df' in locals() and not df.empty:\n",
    "    print(\"üîç Data Quality Assessment:\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_pct = (missing_data / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Missing Data Summary:\")\n",
    "    print(f\"Columns with missing data: {(missing_data > 0).sum()}\")\n",
    "    print(f\"Total missing values: {missing_data.sum():,}\")\n",
    "    print(f\"Missing data percentage: {(missing_data.sum() / (len(df) * len(df.columns))) * 100:.2f}%\")\n",
    "    \n",
    "    # Show columns with highest missing data\n",
    "    if missing_data.sum() > 0:\n",
    "        print(f\"\\nüìâ Top 10 Columns with Missing Data:\")\n",
    "        top_missing = missing_pct[missing_pct > 0].sort_values(ascending=False).head(10)\n",
    "        for col, pct in top_missing.items():\n",
    "            print(f\"  {col}: {pct:.1f}% ({missing_data[col]:,} missing)\")\n",
    "    \n",
    "    # Display basic statistics for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìà Numeric Columns Found: {len(numeric_cols)}\")\n",
    "        print(\"Sample numeric columns:\", list(numeric_cols[:5]))\n",
    "    \n",
    "    # Display categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nüìù Categorical Columns Found: {len(categorical_cols)}\")\n",
    "        print(\"Sample categorical columns:\", list(categorical_cols[:5]))\n",
    "    \n",
    "    # Show data sample\n",
    "    print(f\"\\nüëÄ First 3 Rows Preview:\")\n",
    "    pd.set_option('display.max_columns', 8)\n",
    "    print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79182f67",
   "metadata": {},
   "source": [
    "## Step 3: Quality Assessment using Scout API\n",
    "\n",
    "Let's use Scout's built-in quality assessment endpoint to get a comprehensive quality score for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa54238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Getting quality assessment for: DOT Traffic Speeds NBE\n",
      "‚ùå API request failed: HTTPConnectionPool(host='localhost', port=8080): Read timed out. (read timeout=30)\n",
      "‚ùå Failed to get quality assessment\n"
     ]
    }
   ],
   "source": [
    "# Get quality assessment from Scout API\n",
    "if 'target_dataset_id' in locals():\n",
    "    print(f\"üìã Getting quality assessment for: {target_dataset_name}\")\n",
    "    \n",
    "    quality_data = make_api_request(f\"datasets/{target_dataset_id}/quality\")\n",
    "    \n",
    "    if quality_data:\n",
    "        print(\"‚úÖ Quality assessment completed!\")\n",
    "        \n",
    "        # Display overall quality metrics\n",
    "        print(f\"\\nüéØ Overall Quality Score: {quality_data.get('overall_score', 0):.1f}/100\")\n",
    "        print(f\"üìä Quality Grade: {quality_data.get('grade', 'N/A')}\")\n",
    "        \n",
    "        # Detailed quality metrics\n",
    "        quality_metrics = {\n",
    "            'Completeness': quality_data.get('completeness_score', 0),\n",
    "            'Consistency': quality_data.get('consistency_score', 0),\n",
    "            'Accuracy': quality_data.get('accuracy_score', 0),\n",
    "            'Timeliness': quality_data.get('timeliness_score', 0),\n",
    "            'Usability': quality_data.get('usability_score', 0)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìà Detailed Quality Metrics:\")\n",
    "        for metric, score in quality_metrics.items():\n",
    "            print(f\"  {metric}: {score:.1f}/100\")\n",
    "        \n",
    "        # Missing data percentage\n",
    "        missing_pct = quality_data.get('missing_percentage', 0)\n",
    "        print(f\"\\nüìâ Missing Data: {missing_pct:.2f}%\")\n",
    "        \n",
    "        # Key insights\n",
    "        insights = quality_data.get('insights', [])\n",
    "        if insights:\n",
    "            print(f\"\\nüí° Key Quality Insights:\")\n",
    "            for i, insight in enumerate(insights, 1):\n",
    "                print(f\"  {i}. {insight}\")\n",
    "                \n",
    "        # Create quality visualization\n",
    "        if quality_metrics:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            metrics = list(quality_metrics.keys())\n",
    "            scores = list(quality_metrics.values())\n",
    "            \n",
    "            bars = ax.barh(metrics, scores, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffd93d'])\n",
    "            ax.set_xlabel('Quality Score')\n",
    "            ax.set_title(f'Data Quality Assessment: {target_dataset_name[:50]}...')\n",
    "            ax.set_xlim(0, 100)\n",
    "            \n",
    "            # Add score labels on bars\n",
    "            for bar, score in zip(bars, scores):\n",
    "                ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{score:.1f}', va='center', fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå Failed to get quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035bd439",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis\n",
    "\n",
    "Now let's perform detailed exploratory data analysis on the automobile accident dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2c0893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed exploratory data analysis\n",
    "if 'df' in locals() and not df.empty:\n",
    "    print(\"üîç Starting Detailed EDA...\")\n",
    "    \n",
    "    # Look for key accident-related columns\n",
    "    accident_columns = {\n",
    "        'date': [],\n",
    "        'location': [],\n",
    "        'casualties': [],\n",
    "        'vehicles': [],\n",
    "        'causes': []\n",
    "    }\n",
    "    \n",
    "    # Categorize columns based on common patterns\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(word in col_lower for word in ['date', 'time', 'created', 'occurred']):\n",
    "            accident_columns['date'].append(col)\n",
    "        elif any(word in col_lower for word in ['borough', 'street', 'avenue', 'location', 'address', 'latitude', 'longitude']):\n",
    "            accident_columns['location'].append(col)\n",
    "        elif any(word in col_lower for word in ['injured', 'killed', 'death', 'casualt', 'person']):\n",
    "            accident_columns['casualties'].append(col)\n",
    "        elif any(word in col_lower for word in ['vehicle', 'car', 'truck', 'bike', 'motor']):\n",
    "            accident_columns['vehicles'].append(col)\n",
    "        elif any(word in col_lower for word in ['cause', 'factor', 'reason', 'type']):\n",
    "            accident_columns['causes'].append(col)\n",
    "    \n",
    "    print(\"\\nüìä Column Categories Identified:\")\n",
    "    for category, columns in accident_columns.items():\n",
    "        if columns:\n",
    "            print(f\"\\n{category.title()} Columns ({len(columns)}):\")\n",
    "            for col in columns[:5]:  # Show first 5\n",
    "                print(f\"  ‚Ä¢ {col}\")\n",
    "            if len(columns) > 5:\n",
    "                print(f\"  ... and {len(columns) - 5} more\")\n",
    "    \n",
    "    # Find the most likely date column\n",
    "    date_col = None\n",
    "    for col in accident_columns['date']:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Try to parse as datetime\n",
    "            try:\n",
    "                pd.to_datetime(df[col].head(), errors='raise')\n",
    "                date_col = col\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if date_col:\n",
    "        print(f\"\\nüìÖ Using '{date_col}' as primary date column\")\n",
    "        \n",
    "        # Convert to datetime\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        \n",
    "        # Basic time analysis\n",
    "        valid_dates = df[date_col].dropna()\n",
    "        if len(valid_dates) > 0:\n",
    "            print(f\"Date range: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
    "            print(f\"Valid dates: {len(valid_dates):,} out of {len(df):,} records\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ EDA preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5bb383",
   "metadata": {},
   "source": [
    "## Step 5: Find Related Datasets\n",
    "\n",
    "Let's use Scout's relationship analysis to find datasets related to our automobile accident data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d09ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find related datasets using Scout's relationship analysis\n",
    "if 'target_dataset_id' in locals():\n",
    "    print(f\"üîó Finding datasets related to: {target_dataset_name}\")\n",
    "    \n",
    "    relationship_data = {\n",
    "        \"dataset_id\": target_dataset_id,\n",
    "        \"similarity_threshold\": 0.3,\n",
    "        \"max_related\": 10\n",
    "    }\n",
    "    \n",
    "    relationships = make_api_request(\"datasets/relationships\", method=\"POST\", data=relationship_data)\n",
    "    \n",
    "    if relationships:\n",
    "        related_datasets = relationships.get('related_datasets', [])\n",
    "        network_stats = relationships.get('network_stats', {})\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(related_datasets)} related datasets!\")\n",
    "        \n",
    "        # Display network statistics\n",
    "        print(f\"\\nüìä Network Analysis:\")\n",
    "        print(f\"Total datasets analyzed: {network_stats.get('total_datasets', 0)}\")\n",
    "        print(f\"Relationships found: {network_stats.get('relationships_found', 0)}\")\n",
    "        print(f\"Graph density: {network_stats.get('graph_density', 0):.3f}\")\n",
    "        \n",
    "        # Show top related datasets\n",
    "        if related_datasets:\n",
    "            print(f\"\\nüéØ Top Related Datasets:\")\n",
    "            \n",
    "            for i, dataset in enumerate(related_datasets[:5], 1):\n",
    "                print(f\"\\n{i}. {dataset.get('name', 'Unknown')[:60]}...\")\n",
    "                print(f\"   Similarity Score: {dataset.get('similarity_score', 0):.3f}\")\n",
    "                print(f\"   Category: {dataset.get('category', 'Unknown')}\")\n",
    "                print(f\"   Relationship Reasons: {', '.join(dataset.get('relationship_reasons', []))}\")\n",
    "                \n",
    "            # Create similarity visualization\n",
    "            if len(related_datasets) > 0:\n",
    "                df_related = pd.DataFrame(related_datasets)\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                # Similarity scores plot\n",
    "                plt.subplot(1, 2, 1)\n",
    "                scores = df_related['similarity_score'].head(8)\n",
    "                names = [name[:30] + '...' if len(name) > 30 else name \n",
    "                        for name in df_related['name'].head(8)]\n",
    "                \n",
    "                plt.barh(range(len(scores)), scores, color='skyblue')\n",
    "                plt.yticks(range(len(scores)), names)\n",
    "                plt.xlabel('Similarity Score')\n",
    "                plt.title('Dataset Similarity Scores')\n",
    "                plt.gca().invert_yaxis()\n",
    "                \n",
    "                # Category distribution\n",
    "                plt.subplot(1, 2, 2)\n",
    "                category_counts = df_related['category'].value_counts()\n",
    "                plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "                plt.title('Related Datasets by Category')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "    else:\n",
    "        print(\"‚ùå Failed to find related datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25ad8f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how to use the Scout Data Discovery backend API to:\n",
    "\n",
    "1. **Search for datasets** using multiple search terms\n",
    "2. **Identify the most recently updated** automobile accident dataset\n",
    "3. **Download sample data** for analysis\n",
    "4. **Get comprehensive quality assessments** with detailed metrics\n",
    "5. **Perform exploratory data analysis** with automatic column categorization\n",
    "6. **Find related datasets** using similarity analysis and network metrics\n",
    "\n",
    "### Key Scout API Endpoints Used:\n",
    "\n",
    "- `POST /api/datasets/search` - Search datasets with quality scores\n",
    "- `GET /api/datasets/{id}/sample` - Download dataset samples\n",
    "- `GET /api/datasets/{id}/quality` - Get quality assessments\n",
    "- `POST /api/datasets/relationships` - Find related datasets\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Run individual cells to see the analysis in action\n",
    "- Modify search terms to find other types of datasets\n",
    "- Adjust similarity thresholds for relationship analysis\n",
    "- Explore the Scout frontend at http://localhost:8501 for interactive analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
